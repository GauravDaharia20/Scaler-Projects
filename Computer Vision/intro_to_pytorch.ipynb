{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 5206453.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 776985.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch: 1, Batch: 100, Loss: 1.9742\n",
      "Epoch: 1, Batch: 200, Loss: 0.7044\n",
      "Epoch: 1, Batch: 300, Loss: 0.4767\n",
      "Epoch: 1, Batch: 400, Loss: 0.4055\n",
      "Epoch: 1, Batch: 500, Loss: 0.3639\n",
      "Epoch: 1, Batch: 600, Loss: 0.3266\n",
      "Epoch: 1, Batch: 700, Loss: 0.2714\n",
      "Epoch: 1, Batch: 800, Loss: 0.2703\n",
      "Epoch: 1, Batch: 900, Loss: 0.2613\n",
      "Epoch: 2, Batch: 100, Loss: 0.2133\n",
      "Epoch: 2, Batch: 200, Loss: 0.2187\n",
      "Epoch: 2, Batch: 300, Loss: 0.1955\n",
      "Epoch: 2, Batch: 400, Loss: 0.1916\n",
      "Epoch: 2, Batch: 500, Loss: 0.1916\n",
      "Epoch: 2, Batch: 600, Loss: 0.1770\n",
      "Epoch: 2, Batch: 700, Loss: 0.1554\n",
      "Epoch: 2, Batch: 800, Loss: 0.1507\n",
      "Epoch: 2, Batch: 900, Loss: 0.1606\n",
      "Epoch: 3, Batch: 100, Loss: 0.1449\n",
      "Epoch: 3, Batch: 200, Loss: 0.1449\n",
      "Epoch: 3, Batch: 300, Loss: 0.1466\n",
      "Epoch: 3, Batch: 400, Loss: 0.1247\n",
      "Epoch: 3, Batch: 500, Loss: 0.1271\n",
      "Epoch: 3, Batch: 600, Loss: 0.1435\n",
      "Epoch: 3, Batch: 700, Loss: 0.1326\n",
      "Epoch: 3, Batch: 800, Loss: 0.1368\n",
      "Epoch: 3, Batch: 900, Loss: 0.1165\n",
      "Epoch: 4, Batch: 100, Loss: 0.1186\n",
      "Epoch: 4, Batch: 200, Loss: 0.1101\n",
      "Epoch: 4, Batch: 300, Loss: 0.1138\n",
      "Epoch: 4, Batch: 400, Loss: 0.1173\n",
      "Epoch: 4, Batch: 500, Loss: 0.1133\n",
      "Epoch: 4, Batch: 600, Loss: 0.1065\n",
      "Epoch: 4, Batch: 700, Loss: 0.1017\n",
      "Epoch: 4, Batch: 800, Loss: 0.1037\n",
      "Epoch: 4, Batch: 900, Loss: 0.1022\n",
      "Epoch: 5, Batch: 100, Loss: 0.1001\n",
      "Epoch: 5, Batch: 200, Loss: 0.0939\n",
      "Epoch: 5, Batch: 300, Loss: 0.0876\n",
      "Epoch: 5, Batch: 400, Loss: 0.1041\n",
      "Epoch: 5, Batch: 500, Loss: 0.0854\n",
      "Epoch: 5, Batch: 600, Loss: 0.0972\n",
      "Epoch: 5, Batch: 700, Loss: 0.0925\n",
      "Epoch: 5, Batch: 800, Loss: 0.0930\n",
      "Epoch: 5, Batch: 900, Loss: 0.0985\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Transform for the MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the dataset\n",
    "])\n",
    "\n",
    "# Loading the training data for MNIST\n",
    "train_set = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a simple 4-layer neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),# dense layer with no activation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)  # 10 classes for MNIST digits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layer_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Function to train the network\n",
    "def train_network(train_loader, model, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() # this is where the model learns by backpropagating\n",
    "            optimizer.step() # this is where the model updates its weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Training the model\n",
    "train_network(train_loader, model, criterion, optimizer, epochs=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (2.2.0)\n",
      "Collecting pytorch-lightning\n",
      "  Obtaining dependency information for pytorch-lightning from https://files.pythonhosted.org/packages/56/ed/192d7518b15a06452f480346eeebe1d1d4595af80687e142b2e6f18539fd/pytorch_lightning-2.2.1-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (2.2.0+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (2023.10.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (1.3.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (4.9.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pytorch-lightning) (0.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.8.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "   ---------------------------------------- 0.0/801.6 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/801.6 kB 660.6 kB/s eta 0:00:02\n",
      "   - ------------------------------------- 30.7/801.6 kB 660.6 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 81.9/801.6 kB 919.0 kB/s eta 0:00:01\n",
      "   -------- ----------------------------- 174.1/801.6 kB 952.6 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 225.3/801.6 kB 981.9 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 337.9/801.6 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 491.5/801.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 624.6/801.6 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 778.2/801.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 801.6/801.6 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.0\n",
      "    Uninstalling pytorch-lightning-2.2.0:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.0\n",
      "Successfully installed pytorch-lightning-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | conv1    | Conv2d    | 320   \n",
      "1 | conv2    | Conv2d    | 18.5 K\n",
      "2 | dropout1 | Dropout2d | 0     \n",
      "3 | dropout2 | Dropout2d | 0     \n",
      "4 | fc1      | Linear    | 1.2 M \n",
      "5 | fc2      | Linear    | 1.3 K \n",
      "---------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.800     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e546aff105413dbac7944b89bbc709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648245cb46d64788bb8f6685941d75ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8685d9fa814a6fbebdadb27c337203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "# from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "# import accurayc from torchmetrics\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# if you want to test out something, use keras, for something small or use pytorch for something big\n",
    "\n",
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1) # 1 input channel, 32 output channels, 3x3 kernel, stride of 1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128) # self.fc1 = nn.Linear(linear_unit_1, output_unit)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, task='MULTICLASS', num_classes = 10)\n",
    "        # self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        # self.log('train_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, task='MULTICLASS', num_classes = 10)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "# Define data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train dataset into train and validation sets\n",
    "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "\n",
    "# Define dataloaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "# Initialize model\n",
    "model = MNISTClassifier()\n",
    "\n",
    "# Initialize PyTorch Lightning trainer with corrected GPU argument\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "        pl.callbacks.ModelCheckpoint(monitor='val_loss', filename='best_model', save_top_k=1, mode='min')\n",
    "    ]\n",
    ")\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Test the model\n",
    "# trainer.test(test_dataloaders=test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hydra-core\n",
      "  Obtaining dependency information for hydra-core from https://files.pythonhosted.org/packages/c6/50/e0edd38dcd63fb26a8547f13d28f7a008bc4a3fd4eb4ff030673f22ad41a/hydra_core-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core)\n",
      "  Obtaining dependency information for omegaconf<2.4,>=2.2 from https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ---------------------------------------- 0.0/117.0 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/117.0 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/117.0 kB 325.1 kB/s eta 0:00:01\n",
      "     ------------------- ----------------- 61.4/117.0 kB 465.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 117.0/117.0 kB 680.2 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from hydra-core) (23.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.5 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 112.6/154.5 kB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 154.5/154.5 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.5/79.5 kB 4.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144578 sha256=c63bff822299da4909cf004c9a2d3139bd4a7ef91d8812950ff1bbbfc2d5c275\n",
      "  Stored in directory: c:\\users\\shivam\\appdata\\local\\pip\\cache\\wheels\\23\\cf\\80\\f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install hydra-core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivam\\AppData\\Local\\Temp\\ipykernel_47800\\960703271.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"config\", job_name=\"test_app\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db:\n",
      "  driver: mysql\n",
      "  user: omry\n",
      "  pass: secret\n",
      "secret_key: 123456\n",
      "neural_network:\n",
      "  layer_1:\n",
      "    units: 64\n",
      "    activation: relu\n",
      "  layer_2:\n",
      "    units: 32\n",
      "    activation: relu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize Hydra and compose the configuration\n",
    "initialize(config_path=\"config\", job_name=\"test_app\")\n",
    "cfg = compose(config_name=\"some_config\")\n",
    "\n",
    "# Print the entire config to verify it's loaded\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.neural_network.layer_1.units\n",
    "\n",
    "# use secret key in BOTO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "type(cfg.neural_network.layer_1.units)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyroot utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import pyrootutils\n",
    "root = pyrootutils.setup_root(sys.path[0], pythonpath=True, cwd=True)\n",
    "\n",
    "#setup the path of the module to the parent directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to templates\n",
    "\n",
    "https://github.com/ashleve/lightning-hydra-template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6448536269514722"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "norm.ppf(0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8cc2443dac255f5863d925b738cfe3a24d8333c04bb14f72dfd9c643c8ae38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
