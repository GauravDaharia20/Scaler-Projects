{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use L1/L2/Elastic Net Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing between L1, L2, and Elastic Net regularization involves considering the specific characteristics of your dataset, your model's requirements, and the computational resources at your disposal. Here's a detailed summary of when to use each, along with their pros and cons:\n",
    "\n",
    "### L1 Regularization (Lasso)\n",
    "\n",
    "**When to Use:**\n",
    "- When you have a high-dimensional dataset with many features, but suspect only a few are actually important (sparse feature set).\n",
    "- When model interpretability is important, as L1 can zero out the coefficients of less important features, effectively performing feature selection.\n",
    "\n",
    "**Pros:**\n",
    "- Encourages sparsity, thus performing implicit feature selection.\n",
    "- Can help improve model interpretability by eliminating irrelevant features.\n",
    "\n",
    "**Cons:**\n",
    "- Can be unstable in the presence of highly correlated features, arbitrarily selecting one feature over another.\n",
    "- Not well-suited for situations where many small or correlated features contribute to the outcome.\n",
    "\n",
    "### L2 Regularization (Ridge)\n",
    "\n",
    "**When to Use:**\n",
    "- When dealing with multicollinearity (i.e., when independent variables are highly correlated).\n",
    "- When you expect that many small or moderate effects contribute to the outcome, rather than a few variables with large effects.\n",
    "\n",
    "**Pros:**\n",
    "- Tends to give better prediction accuracy than L1 when features are correlated.\n",
    "- Stabilizes the estimation of regression coefficients by penalizing the size of coefficients without setting them to zero.\n",
    "\n",
    "**Cons:**\n",
    "- Does not perform feature selection — all features are included in the model, which can make the model complex and harder to interpret.\n",
    "- Might not be as effective in high-dimensional spaces where sparsity is preferable.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "**When to Use:**\n",
    "- When you want to combine the benefits of L1 and L2 regularization.\n",
    "- In the presence of correlated features, when you also need to perform feature selection.\n",
    "- When dealing with high-dimensional data where L1 regularization might select too few features, or L2 regularization does not provide enough regularization.\n",
    "\n",
    "**Pros:**\n",
    "- Balances between feature selection (L1) and regularization (L2), making it versatile for various scenarios.\n",
    "- Can outperform L1 and L2 regularization alone in many cases, especially with datasets that have multiple correlated features.\n",
    "\n",
    "**Cons:**\n",
    "- Adds an extra layer of complexity in choosing the regularization parameters (requires tuning two hyperparameters).\n",
    "- Computationally more intensive due to the need for parameter tuning.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **L1 Regularization** is best suited for scenarios where sparsity and feature selection are crucial.\n",
    "- **L2 Regularization** is ideal for problems with multicollinearity or when all features are expected to contribute towards the prediction.\n",
    "- **Elastic Net** offers a middle ground, leveraging the strengths of both L1 and L2 regularization, making it a powerful choice for complex datasets, albeit at the cost of increased computational complexity and the need for more sophisticated hyperparameter tuning.\n",
    "\n",
    "The choice among L1, L2, and Elastic Net should be guided by cross-validation to determine which method offers the best performance for your specific dataset and problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Dropout Layers Are Effective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique used in neural networks that helps prevent overfitting. The intuition behind how dropout acts as a regularizer can be understood through its operational mechanism and the effects it has on the training process and the model's generalization capability. Here’s a breakdown to build that intuition:\n",
    "\n",
    "### Mechanism of Dropout\n",
    "\n",
    "- **Random Deactivation**: During training, dropout randomly deactivates a proportion of neurons (i.e., units) in the network at each training step or epoch. This means that a randomly selected subset of neurons does not contribute to the forward pass (calculation of the output) and does not get updated during the backpropagation step.\n",
    "- **Dynamic Network Thinning**: By deactivating different subsets of neurons at each training step, dropout effectively trains a \"thinned\" version of the network. Since the deactivated neurons change every time, it’s like training a large ensemble of smaller, different networks.\n",
    "\n",
    "### Effects and Intuition\n",
    "\n",
    "1. **Prevention of Co-Adaptations**: Dropout prevents neurons from co-adapting too closely. When neurons rely on the presence of specific other neurons to correct their mistakes, the network can become overly complex and specialized to the training data, leading to overfitting. Dropout ensures that neurons cannot rely on the presence of others, forcing them to become more robust and capable of making correct decisions independently.\n",
    "\n",
    "2. **Ensemble Interpretation**: The process of randomly dropping out neurons during training can be seen as training a large number of different \"thinned\" networks with shared weights. At test time, using the full network with scaled-down weights is akin to averaging the predictions of these thinned networks, similar to ensemble methods like bagging. Ensemble methods are known for their ability to improve model robustness and generalization.\n",
    "\n",
    "3. **Effective Capacity Reduction**: Dropout reduces the effective capacity of the network by limiting the number of active neurons during training. This reduction in capacity helps prevent the network from overfitting by ensuring it cannot memorize the training data too closely.\n",
    "\n",
    "4. **Noise Introduction**: Dropout introduces noise into the training process, which can be beneficial for preventing overfitting. This noise forces the network to learn more robust features that are invariant to the presence or absence of particular neurons, thereby improving the model's generalization to unseen data.\n",
    "\n",
    "5. **Regularization without Explicit Constraints**: Unlike traditional regularization methods that add explicit constraints to the loss function (e.g., L1 or L2 penalties), dropout regularizes the model implicitly. It encourages the learning of more generalized representations without directly manipulating the loss function's form.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The intuitive understanding of dropout as a regularizer comes from its ability to prevent complex co-adaptations among neurons, simulate an ensemble-like effect by averaging over many thinned networks, reduce the model's effective capacity, and introduce beneficial noise into the training process. These factors contribute to the development of a more robust and generalizable model that performs better on unseen data, effectively serving the purpose of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# !gdown 1mMRZKe5Qm99fJBE9y0mLdvVZYDfHvkxY\n",
    "\n",
    "df = pd.read_csv('Amazon.csv', encoding='latin-1')\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category-encoders\n",
      "  Obtaining dependency information for category-encoders from https://files.pythonhosted.org/packages/7f/e5/79a62e5c9c9ddbfa9ff5222240d408c1eeea4e38741a0dc8343edc7ef1ec/category_encoders-2.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (1.11.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (0.14.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (2.0.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from category-encoders) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from scikit-learn>=0.20.0->category-encoders) (2.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\shivam\\miniconda3\\envs\\mlops\\lib\\site-packages (from statsmodels>=0.9.0->category-encoders) (23.1)\n",
      "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.9 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/81.9 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 61.4/81.9 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.9/81.9 kB 918.2 kB/s eta 0:00:00\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install category-encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  (7039, 10) (7039,)\n",
      "Validation: (1760, 10) (1760,)\n",
      "Test  :  (2200, 10) (2200,)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mValidation:\u001b[39m\u001b[39m'\u001b[39m, X_val\u001b[39m.\u001b[39mshape, y_val\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest  : \u001b[39m\u001b[39m'\u001b[39m, X_test\u001b[39m.\u001b[39mshape, y_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcategory_encoders\u001b[39;00m \u001b[39mimport\u001b[39;00m TargetEncoder\n\u001b[0;32m     20\u001b[0m enc \u001b[39m=\u001b[39m TargetEncoder(cols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mWarehouse_block\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMode_of_Shipment\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mProduct_importance\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mGender\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m X_train \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39mfit_transform(X_train, y_train)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['ID','Returned'])\n",
    "y = df['Returned']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train : ', X_train.shape, y_train.shape)\n",
    "print('Validation:', X_val.shape, y_val.shape)\n",
    "print('Test  : ', X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Warehouse_block</th>\n",
       "      <th>Mode_of_Shipment</th>\n",
       "      <th>Customer_care_calls</th>\n",
       "      <th>Customer_rating</th>\n",
       "      <th>Cost_of_the_Product</th>\n",
       "      <th>Prior_purchases</th>\n",
       "      <th>Product_importance</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Discount_offered</th>\n",
       "      <th>Weight_in_gms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>0.578005</td>\n",
       "      <td>0.608167</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600878</td>\n",
       "      <td>0.596148</td>\n",
       "      <td>10</td>\n",
       "      <td>5180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7746</th>\n",
       "      <td>0.600336</td>\n",
       "      <td>0.600251</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>228</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600878</td>\n",
       "      <td>0.599487</td>\n",
       "      <td>9</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>0.601109</td>\n",
       "      <td>0.608167</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>4</td>\n",
       "      <td>0.586928</td>\n",
       "      <td>0.596148</td>\n",
       "      <td>41</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>0.601109</td>\n",
       "      <td>0.600251</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>221</td>\n",
       "      <td>10</td>\n",
       "      <td>0.586928</td>\n",
       "      <td>0.596148</td>\n",
       "      <td>42</td>\n",
       "      <td>2972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10404</th>\n",
       "      <td>0.600336</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>243</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600878</td>\n",
       "      <td>0.599487</td>\n",
       "      <td>1</td>\n",
       "      <td>1856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Warehouse_block  Mode_of_Shipment  Customer_care_calls  \\\n",
       "10286         0.578005          0.608167                    6   \n",
       "7746          0.600336          0.600251                    4   \n",
       "1789          0.601109          0.608167                    5   \n",
       "2521          0.601109          0.600251                    6   \n",
       "10404         0.600336          0.576471                    5   \n",
       "\n",
       "       Customer_rating  Cost_of_the_Product  Prior_purchases  \\\n",
       "10286                2                  196                2   \n",
       "7746                 3                  228                5   \n",
       "1789                 2                  231                4   \n",
       "2521                 4                  221               10   \n",
       "10404                3                  243                6   \n",
       "\n",
       "       Product_importance    Gender  Discount_offered  Weight_in_gms  \n",
       "10286            0.600878  0.596148                10           5180  \n",
       "7746             0.600878  0.599487                 9           1044  \n",
       "1789             0.586928  0.596148                41           2992  \n",
       "2521             0.586928  0.596148                42           2972  \n",
       "10404            0.600878  0.599487                 1           1856  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "\n",
    "enc = TargetEncoder(cols=['Warehouse_block','Mode_of_Shipment','Product_importance','Gender'])\n",
    "X_train = enc.fit_transform(X_train, y_train)\n",
    "\n",
    "X_val = enc.transform(X_val, y_val)\n",
    "X_test = enc.transform(X_test, y_test)\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shivam\\miniconda3\\envs\\mlops\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# For Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shivam\\miniconda3\\envs\\mlops\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Shivam\\miniconda3\\envs\\mlops\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Shivam\\miniconda3\\envs\\mlops\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 0.5585 - accuracy: 0.6420 - val_loss: 0.5523 - val_accuracy: 0.6375\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.6723 - val_loss: 0.5518 - val_accuracy: 0.6420\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.6816 - val_loss: 0.5469 - val_accuracy: 0.6392\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.6812 - val_loss: 0.5418 - val_accuracy: 0.6432\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.6872 - val_loss: 0.5436 - val_accuracy: 0.6267\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.6894 - val_loss: 0.5433 - val_accuracy: 0.6466\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.6929 - val_loss: 0.5473 - val_accuracy: 0.6381\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7007 - val_loss: 0.5482 - val_accuracy: 0.6324\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7018 - val_loss: 0.5418 - val_accuracy: 0.6449\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7025 - val_loss: 0.5456 - val_accuracy: 0.6381\n"
     ]
    }
   ],
   "source": [
    "def create_baseline():\n",
    "\n",
    "  model = Sequential([\n",
    "                    Dense(256, activation=\"relu\"),\n",
    "                    Dense(128, activation=\"relu\"),\n",
    "                    Dense(64, activation=\"relu\"),\n",
    "                    Dense(1 , activation = 'sigmoid')])\n",
    "  return model\n",
    "\n",
    "\n",
    "model = create_baseline()\n",
    "\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data = (X_val, y_val),  epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 753us/step - loss: 0.4827 - accuracy: 0.7173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48271700739860535, 0.717289388179779]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 803us/step - loss: 0.5456 - accuracy: 0.6381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5455969572067261, 0.6380681991577148]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l1/l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 4ms/step - loss: 0.5572 - accuracy: 0.6448 - val_loss: 0.5502 - val_accuracy: 0.6369\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.6772 - val_loss: 0.5540 - val_accuracy: 0.6432\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.6792 - val_loss: 0.5462 - val_accuracy: 0.6369\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.6809 - val_loss: 0.5432 - val_accuracy: 0.6443\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.6822 - val_loss: 0.5422 - val_accuracy: 0.6267\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.6914 - val_loss: 0.5456 - val_accuracy: 0.6392\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.6924 - val_loss: 0.5491 - val_accuracy: 0.6403\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.6957 - val_loss: 0.5520 - val_accuracy: 0.6324\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7034 - val_loss: 0.5433 - val_accuracy: 0.6426\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7039 - val_loss: 0.5458 - val_accuracy: 0.6381\n"
     ]
    }
   ],
   "source": [
    "def create_baseline():\n",
    "    # lambda = 0.01\n",
    "    L2Reg = tf.keras.regularizers.L2(l2=1e-6)\n",
    "    model = Sequential([\n",
    "                    Dense(256, activation=\"relu\", kernel_regularizer = L2Reg ),\n",
    "                    Dense(128, activation=\"relu\", kernel_regularizer = L2Reg),\n",
    "                    Dense(64, activation=\"relu\", kernel_regularizer = L2Reg),\n",
    "                    Dense(1 , activation = 'sigmoid')])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_baseline()\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data = (X_val, y_val),  epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 772us/step - loss: 0.4844 - accuracy: 0.7144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4843883812427521, 0.714448094367981]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 777us/step - loss: 0.5458 - accuracy: 0.6381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5457697510719299, 0.6380681991577148]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.6403 - val_loss: 0.5555 - val_accuracy: 0.6392\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5271 - accuracy: 0.6640 - val_loss: 0.5479 - val_accuracy: 0.6455\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.6703 - val_loss: 0.5401 - val_accuracy: 0.6449\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5172 - accuracy: 0.6684 - val_loss: 0.5393 - val_accuracy: 0.6403\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.6758 - val_loss: 0.5405 - val_accuracy: 0.6415\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.6774 - val_loss: 0.5412 - val_accuracy: 0.6460\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.6835 - val_loss: 0.5382 - val_accuracy: 0.6506\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.6816 - val_loss: 0.5414 - val_accuracy: 0.6517\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.6795 - val_loss: 0.5392 - val_accuracy: 0.6472\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.6782 - val_loss: 0.5390 - val_accuracy: 0.6403\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "def create_Dropout():\n",
    "    # lambda = 0.01\n",
    "    L2Reg = tf.keras.regularizers.L2(l2=1e-6)\n",
    "    model = Sequential([\n",
    "                    Dense(256, activation=\"relu\", kernel_regularizer = L2Reg ),\n",
    "                    Dropout(0.3),\n",
    "                    Dense(128, activation=\"relu\", kernel_regularizer = L2Reg),\n",
    "                    Dropout(0.3),\n",
    "                    Dense(64, activation=\"relu\", kernel_regularizer = L2Reg),\n",
    "                    Dense(1 , activation = 'sigmoid')])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_Dropout()\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "                  loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data = (X_val, y_val),  epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 757us/step - loss: 0.5028 - accuracy: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5027924180030823, 0.6931382417678833]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 778us/step - loss: 0.5390 - accuracy: 0.6403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5389804244041443, 0.6403409242630005]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "55/55 [==============================] - 1s 5ms/step - loss: 4.3012 - accuracy: 0.5693 - val_loss: 3.4693 - val_accuracy: 0.4750\n",
      "Epoch 2/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.2966 - accuracy: 0.6312 - val_loss: 2.0457 - val_accuracy: 0.5682\n",
      "Epoch 3/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.9694 - accuracy: 0.6272 - val_loss: 1.6609 - val_accuracy: 0.6074\n",
      "Epoch 4/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.7300 - accuracy: 0.6359 - val_loss: 1.4039 - val_accuracy: 0.6057\n",
      "Epoch 5/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.5167 - accuracy: 0.6272 - val_loss: 1.9091 - val_accuracy: 0.5926\n",
      "Epoch 6/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.7208 - accuracy: 0.6141 - val_loss: 2.0787 - val_accuracy: 0.6034\n",
      "Epoch 7/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.0866 - accuracy: 0.6038 - val_loss: 2.3186 - val_accuracy: 0.6051\n",
      "Epoch 8/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.2887 - accuracy: 0.6191 - val_loss: 2.0558 - val_accuracy: 0.5898\n",
      "Epoch 9/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.5583 - accuracy: 0.6049 - val_loss: 3.0973 - val_accuracy: 0.6091\n",
      "Epoch 10/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.4515 - accuracy: 0.5989 - val_loss: 3.3755 - val_accuracy: 0.6193\n",
      "Epoch 11/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.2936 - accuracy: 0.6099 - val_loss: 3.2988 - val_accuracy: 0.6193\n",
      "Epoch 12/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.5045 - accuracy: 0.5862 - val_loss: 3.1392 - val_accuracy: 0.6102\n",
      "Epoch 13/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.4469 - accuracy: 0.5953 - val_loss: 3.4568 - val_accuracy: 0.5926\n",
      "Epoch 14/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 3.0867 - accuracy: 0.6012 - val_loss: 2.9027 - val_accuracy: 0.5926\n",
      "Epoch 15/15\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 2.7816 - accuracy: 0.6120 - val_loss: 2.8607 - val_accuracy: 0.5932\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "def create_BatchNormalization_model():\n",
    "    L2Reg = tf.keras.regularizers.L2(l2=1e-6)\n",
    "    model = Sequential([\n",
    "                    Dense(256, kernel_regularizer = L2Reg),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(relu),\n",
    "                    Dropout(0.2),\n",
    "                    Dense(128, kernel_regularizer = L2Reg),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(relu),\n",
    "                    Dense(64,kernel_regularizer = L2Reg ),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(relu),\n",
    "                    Dense(1)])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_BatchNormalization_model()\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data = (X_val, y_val),  epochs=15, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 256)               2816      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45825 (179.00 KB)\n",
      "Trainable params: 44929 (175.50 KB)\n",
      "Non-trainable params: 896 (3.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/2 + 512/2 + 1024/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 846us/step - loss: 2.7145 - accuracy: 0.6161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.714524984359741, 0.6161386370658875]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 852us/step - loss: 2.8607 - accuracy: 0.5932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.860673189163208, 0.5931817889213562]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8cc2443dac255f5863d925b738cfe3a24d8333c04bb14f72dfd9c643c8ae38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
