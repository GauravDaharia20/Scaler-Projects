{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, InputLayer, RNN, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"flipitnews-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Article\n",
       "0     Technology  tv future in the hands of viewers with home th...\n",
       "1       Business  worldcom boss  left books alone  former worldc...\n",
       "2         Sports  tigers wary of farrell  gamble  leicester say ...\n",
       "3         Sports  yeading face newcastle in fa cup premiership s...\n",
       "4  Entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')\n",
    "stopword.extend(punctuation)\n",
    "def get_text_preprocessed(words):\n",
    "    words = words.lower()\n",
    "    words = re.sub(r'[^a-zA-Z\\s]', '', words)\n",
    "    words = word_tokenize(words)\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words if word not in stopword]\n",
    "    words = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processd_article'] = df['Article'].apply(get_text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['Category_cat'] = LabelEncoder().fit_transform(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['processd_article'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['Category_cat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "      <th>processd_article</th>\n",
       "      <th>Category_cat</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>4</td>\n",
       "      <td>2801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "      <td>3</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "      <td>3</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "      <td>1</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Article  \\\n",
       "0     Technology  tv future in the hands of viewers with home th...   \n",
       "1       Business  worldcom boss  left books alone  former worldc...   \n",
       "2         Sports  tigers wary of farrell  gamble  leicester say ...   \n",
       "3         Sports  yeading face newcastle in fa cup premiership s...   \n",
       "4  Entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                    processd_article  Category_cat  length  \n",
       "0  tv future hand viewer home theatre system plas...             4    2801  \n",
       "1  worldcom bos left book alone former worldcom b...             0    1320  \n",
       "2  tiger wary farrell gamble leicester say rushed...             3     850  \n",
       "3  yeading face newcastle fa cup premiership side...             3    1789  \n",
       "4  ocean twelve raid box office ocean twelve crim...             1    1130  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for val in df['processd_article']:\n",
    "    corpus.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15827"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df['length'].max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_words(text_col):\n",
    "\n",
    "    # Prepare a tokenizer on testing data\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(list(text_col))\n",
    "\n",
    "    thresh = 5\n",
    "\n",
    "    cnt = 0\n",
    "    tot_cnt = 0\n",
    "\n",
    "    for key, value in text_tokenizer.word_counts.items():\n",
    "        tot_cnt = tot_cnt + 1\n",
    "        if value < thresh:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "    return cnt, tot_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1780,), (445,), (1780,), (445,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(np.array(corpus),\n",
    "                                            np.array(df[\"Category_cat\"]),\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=0,\n",
    "                                            shuffle=True\n",
    "                                           )\n",
    "\n",
    "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 66.02579604997985\n"
     ]
    }
   ],
   "source": [
    "x_train_cnt, x_train_tot_cnt = get_rare_words(text_col=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 8430\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words=x_train_tot_cnt - x_train_cnt)\n",
    "# x_tokenizer = Tokenizer(num_words = x_train_tot_cnt)\n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_train) # please save tokenizer when you train model\n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_valid)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen=100, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=100, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_vector_size = 500\n",
    "model = Sequential([\n",
    "    Embedding(x_voc,Embedding_vector_size,input_length=100,trainable=True),\n",
    "    LSTM(15,return_sequences=True,dropout=0.4),\n",
    "    LSTM(10,return_sequences=False,dropout=0.6),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(set(df['Category_cat'])),activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 500)          4215000   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100, 15)           30960     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100, 15)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 10)                1040      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,247,055\n",
      "Trainable params: 4,247,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_backs = EarlyStopping(monitor='val_loss',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "56/56 [==============================] - 7s 58ms/step - loss: 1.3786 - accuracy: 0.4713 - val_loss: 1.0956 - val_accuracy: 0.5146\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.8109 - accuracy: 0.7910 - val_loss: 0.6172 - val_accuracy: 0.8854\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.3947 - accuracy: 0.9725 - val_loss: 0.4244 - val_accuracy: 0.9213\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 0.2207 - accuracy: 0.9910 - val_loss: 0.3326 - val_accuracy: 0.9326\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.1434 - accuracy: 0.9961 - val_loss: 0.2962 - val_accuracy: 0.9303\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.0946 - accuracy: 0.9978 - val_loss: 0.2975 - val_accuracy: 0.9146\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.0780 - accuracy: 0.9955 - val_loss: 0.3308 - val_accuracy: 0.9056\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 0.0786 - accuracy: 0.9876 - val_loss: 0.2603 - val_accuracy: 0.9348\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.0650 - accuracy: 0.9927 - val_loss: 0.3529 - val_accuracy: 0.9011\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 2s 37ms/step - loss: 0.0441 - accuracy: 0.9972 - val_loss: 0.3139 - val_accuracy: 0.9124\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 0.0334 - accuracy: 0.9994 - val_loss: 0.3131 - val_accuracy: 0.9146\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.3077 - val_accuracy: 0.9191\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.3060 - val_accuracy: 0.9258\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9258\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3258 - val_accuracy: 0.9258\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9191\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.3630 - val_accuracy: 0.9213\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 0.0192 - accuracy: 0.9978 - val_loss: 0.3014 - val_accuracy: 0.9371\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_tr,y_train,validation_data=(x_val,y_valid),batch_size=32,epochs=50,verbose=1,callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
